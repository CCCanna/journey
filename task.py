from collections import Counter

from database import *
import parser


def get_zero_clock(date):
    """"è·å–å¯¹åº”æ—¥æœŸé›¶ç‚¹çš„æ—¶é—´æˆ³"""
    return datetime.datetime.strptime("{} 00:00:00".format(date), "%Y-%m-%d %H:%M:%S")


def get_week_index():
    """ç”Ÿæˆä¸€ä¸ªåˆ—è¡¨ï¼Œè¿”å›æ¯ä¸ªæ˜ŸæœŸä¸€é›¶ç‚¹çš„æ—¶é—´æˆ³"""
    MIN, MAX = log_data.unix_time.min(), log_data.unix_time.max()
    weeks = list()
    a_week = 3600 * 24 * 7
    current = get_first_weekday(MIN)
    while current < MAX:
        weeks.append(current)
        current = current + a_week
    # è¡¥ä¸Šæœ€åä¸€å¤©
    weeks.append(MAX)
    return shrink(weeks)


def get_first_weekday(day):
    """è·å–ä¸‹ä¸€ä¸ªæ˜ŸæœŸä¸€çš„æ—¶é—´æˆ³ï¼Œä»Šå¤©æ˜¯å‘¨ä¸€åˆ™è¿”å›ä»Šå¤©"""
    current = datetime.datetime.fromtimestamp(day)
    if current.weekday():
        return get_zero_clock((current + datetime.timedelta(days=7 - current.weekday())).date()).timestamp()
    return get_zero_clock(current.date()).timestamp()


def shrink(original_list):
    """"æ¶ˆé™¤åˆ—è¡¨ä¸­çš„é‡å¤å…ƒç´ ï¼Œå“å‘€ç”¨setä¸å¥½ä½¿æ¬¸"""
    repeat = Counter(original_list)
    return sorted(repeat.keys())


def worker(user_id):
    """é›†åˆæ‰€éœ€è¦çš„æ‰€æœ‰æ•°æ®"""
    try:
        # ä¹‹æ‰€ä»¥ä¼štryæ˜¯å› ä¸ºopenidé‡Œé¢æœ‰ä¸ªç©ºå€¼ï¼Œä¸è¿‡è¿™æ ·å†™æ„Ÿè§‰å¹¶ä¸å¥½
        results = log_data.query("openid=='{}'".format(user_id))
    except SyntaxError:
        return
    _, ip, unix_time, something = transform_result_set(results)
    user_ip, *_ = shrink(ip)
    user = dict()
    user['identifier'] = user_id
    user['ip'] = user_ip
    # æŠŠæ—¶é—´æˆ³å’Œå¯¹åº”çš„è¡Œä¸ºè½¬æ¢æˆå­—å…¸å†å¯¼å‡ºä¸ºjsonå­—ç¬¦ä¸²
    user['activities'] = json.dumps(dict(zip(unix_time, something)))
    time_series = simplify_time(shrink(unix_time))
    dates = [datetime.datetime.fromtimestamp(x).date().isoformat() for x in time_series]
    data_counter = Counter(dates)
    user['time_json'] = json.dumps(data_counter)
    pool = [user.get(key) for key in ['identifier', 'ip', 'activities', 'time_json']]
    pool.extend(count_activities(data_counter, time_series))
    return pool


def simplify_time(time_series):
    """æŒ‰ç…§1å°æ—¶çš„ç•Œé™åˆ’åˆ†ç”¨æˆ·æ“ä½œæ—¶é—´"""
    simplified = list()
    index = 0
    while True:
        simplified.append(time_series[index])
        index = search_for_nearest(time_series, time_series[index] + 3600)
        if not index:
            break
    return simplified


def search_for_nearest(series, value):
    """è·å–åˆ—è¡¨ä¸­å¤§äºç•Œé™çš„æœ€å°å€¼"""
    distance = [serial - value for serial in series]
    for index, val in enumerate(distance):
        if val >= 0:
            return index
    return 0


def weekday_map(series):
    """ç”Ÿæˆä¸€ä¸ªå¯¹åº”æ¯ä¸ªæ˜ŸæœŸçš„è®¿é—®æ¬¡æ•°å­—å…¸"""
    result = dict()
    for index, limit in enumerate(weeks_index):
        count = 0
        while series:
            if series[0] <= limit:
                count += 1
                series.pop(0)
            else:
                break
        result[index] = count
    return result


def count_activities(counter, series):
    """ç»Ÿè®¡DAU,WAU,MAU"""
    monthly = sum(counter.values())
    daily = list(counter.values()).pop(0)
    weekly = max(sorted(weekday_map(series).values()))
    return daily, weekly, monthly


def expand_openid():
    """è·å–ç”¨æˆ·openidåˆ—è¡¨"""
    frame = query_for_result("select distinct openid from user_log;")
    return transform_result_set(frame)[0]


# # äººå®¶æƒ³æŠŠç¬¬äºŒæ­¥çš„ä»£ç ç»™åšæˆä¸€ä¸ªæ¨¡å—æ¥è€…ï¼Œç„¶è€Œå‘ç°log_dataç‹¬ç«‹ä¸å‡ºå»ã€‚
# # è€Œä¸”ç¬¬ä¸€æ­¥ç”Ÿæˆçš„DataFrameä¸èƒ½ç›´æ¥åœ¨ç¬¬äºŒæ­¥ä½¿ç”¨ï¼Œä»¥åŠè„šæœ¬æ˜¯åŸºäºSQLæ•°æ®åº“çš„ï¼Œå¯¹äºCSVç”¨æˆ·ä¸æ˜¯å¾ˆå‹å¥½(è¿™æ˜¯ç¼–ç çš„è®¾(è„‘)è®¡(å­)ç¼º(æœ‰)é™·(å‘)
# parser.main()
# print("step 2 running... it may take 8 minutes.")
# create_table_user_action()
# # weeks_indexè¿™ä¸ªå˜é‡ä¼šåœ¨ç¬¬ä¸‰æ­¥è¢«ç”¨åˆ°ï¼Œç”Ÿæˆè¿™ä¸ªå˜é‡éœ€è¦ç¬¬ä¸€æ­¥çš„æ•°æ®
# log_data = fetch_log_data()
# weeks_index = get_week_index()
# result_set = list()
#
# for openid in expand_openid():
#     result_set.append(worker(openid))
#
# data_header = ['openid', 'ip', 'activities', 'time_json', 'dau', 'wau', 'mau']
# data_frame = pd.DataFrame(result_set, columns=data_header)
# # å­˜æˆcsvã€‚ã€‚ã€‚åé¢çš„æ•°æ®åº“è€æ˜¯æŠ¥é”™æ€ä¹ˆåŠï¼Œæˆ‘ä¹Ÿå¾ˆç»æœ›å•Š
# data_frame.to_csv(os.path.join(data_dir, 'user_action.csv'), index=None)
# data_frame.to_sql('user_action', con=engine, chunksize=1000, if_exists='append', index=None)
# print("calculation finished.")


weeks_index = [1557072000.0, 1557676800.0, 1558281600.0, 1558886400.0, 1559491200.0, 1560096000.0, 1560700800.0,
               1561127751]
# æ‰§è¡Œæœ€åä¸€æ­¥æ“ä½œç”ŸæˆæŠ¥è¡¨(ç”»å›¾ä»€ä¹ˆçš„æ˜¯ä¸å¯èƒ½çš„
print("working on step 3, will output needed data in csv format.")
# æ´»è·ƒç”¨æˆ·æŒ‡æ ‡&&æ¯ä¸ªç”¨æˆ·æ€»æ´»è·ƒå¤©æ•°æŒ‡æ ‡
build_csv("DAUæ•°æ®.csv", ['æ´»è·ƒå¤©æ•°', 'äººæ•°'], 'select dau,count(*) from user_action group by dau')
build_csv("WAUæ•°æ®.csv", ['æ´»è·ƒå¤©æ•°', 'äººæ•°'], 'select wau,count(*) from user_action group by wau')
build_csv("MAUæ•°æ®.csv", ['æ´»è·ƒå¤©æ•°', 'äººæ•°'], 'select mau,count(*) from user_action group by mau')

# å¯åŠ¨æ¬¡æ•°æŒ‡æ ‡
# ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­æœ€å¤§çš„è®¾è®¡é—®é¢˜ï¼Œæ²¡äº‹åšå¾€æ•°æ®åº“é‡Œé¢å­˜ä¸¤ä¸‡ä¸ªjson
# è¿™æ˜¯è¦è¢«æ‰“æ­»çš„èŠ‚å¥å•ŠğŸ˜±ğŸ˜±ğŸ˜±ï¼Œå•Šäº§å“å§å§è½»ç‚¹
json_data = merge_json(transform_result_set(query_for_result('select time_json from user_action'))[0])
convert_dict(json_data, 'æ¯æ—¥ç”¨æˆ·æ´»è·ƒæ•°.csv')
convert_dict(build_weeks_json(weeks_index, json_data), 'æ¯å‘¨ç”¨æˆ·æ´»è·ƒæ•°.csv')

# ç”¨æˆ·è¡Œä¸ºåˆ†æ
# ã€‚ã€‚ã€‚ã€‚
# æ•°æ®å­˜åœ¨é—®é¢˜ï¼Œä¸åˆ†æå•¦
